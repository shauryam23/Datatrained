{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3dd1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data has been saved to q1.txt.\n"
     ]
    }
   ],
   "source": [
    "#Question 1 \n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "try:\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the table containing the most viewed videos data\n",
    "        table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "        # Initialize lists to store the scraped data\n",
    "        ranks = []\n",
    "        names = []\n",
    "        artists = []\n",
    "        upload_dates = []\n",
    "        views = []\n",
    "\n",
    "        # Iterate through the rows of the table to extract data\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "\n",
    "            # Check if the row contains enough columns\n",
    "            if len(columns) >= 5:\n",
    "                ranks.append(columns[0].text.strip())\n",
    "                names.append(columns[1].text.strip())\n",
    "                artists.append(columns[2].text.strip())\n",
    "                upload_dates.append(columns[4].text.strip())\n",
    "                views.append(columns[3].text.strip())\n",
    "\n",
    "        # Create and write the scraped data to a text file with encoding specified\n",
    "        with open('q1.txt', 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(ranks)):\n",
    "                file.write(f\"Rank: {ranks[i]}\\n\")\n",
    "                file.write(f\"Name: {names[i]}\\n\")\n",
    "                file.write(f\"Artist: {artists[i]}\\n\")\n",
    "                file.write(f\"Upload Date: {upload_dates[i]}\\n\")\n",
    "                file.write(f\"Views: {views[i]}\\n\")\n",
    "                file.write(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "        print(\"Scraped data has been saved to q1.txt.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page.\")\n",
    "\n",
    "except requests.exceptions.RequestException as req_exception:\n",
    "    print(f\"Request Exception: {req_exception}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38d2955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data has been saved to q2.txt.\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "\n",
    "# Define the URL of the \"International Fixtures\" page\n",
    "url = \"https://www.bcci.tv/international/fixtures\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Locate and extract the information with error handling\n",
    "    match_title_elem = soup.find(\"div\", class_=\"match-title\")\n",
    "    series_elem = soup.find(\"span\", class_=\"series-name\")\n",
    "    place_elem = soup.find(\"strong\", class_=\"venue\")\n",
    "    date_elem = soup.find(\"strong\", class_=\"match-date\")\n",
    "    time_elem = soup.find(\"strong\", class_=\"match-time\")\n",
    "\n",
    "    # Check if the elements were found before accessing their text\n",
    "    match_title = match_title_elem.text.strip() if match_title_elem else \"Match title not found\"\n",
    "    series = series_elem.text.strip() if series_elem else \"Series not found\"\n",
    "    place = place_elem.text.strip() if place_elem else \"Place not found\"\n",
    "    date = date_elem.text.strip() if date_elem else \"Date not found\"\n",
    "    time = time_elem.text.strip() if time_elem else \"Time not found\"\n",
    "\n",
    "    # Create or open the q2.txt file and write the extracted information\n",
    "    with open(\"q2.txt\", \"w\") as file:\n",
    "        file.write(\"Match Title: \" + match_title + \"\\n\")\n",
    "        file.write(\"Series: \" + series + \"\\n\")\n",
    "        file.write(\"Place: \" + place + \"\\n\")\n",
    "        file.write(\"Date: \" + date + \"\\n\")\n",
    "        file.write(\"Time: \" + time + \"\\n\")\n",
    "\n",
    "    print(\"Scraped data has been saved to q2.txt.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "871a398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table containing GDP data not found on the page.\n"
     ]
    }
   ],
   "source": [
    "#Question 3 \n",
    "# Define the URL of the \"Economy - India\" page\n",
    "url = \"https://www.statisticstimes.com/economy/india-statistics.php\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Assuming you've identified the specific page containing the GDP data,\n",
    "    # find the relevant table and extract the data\n",
    "    # Replace \"table_selector\" with the actual CSS selector for the table\n",
    "    table_selector = \"#your-selector-for-the-table\"\n",
    "    data_table = soup.select_one(table_selector)\n",
    "\n",
    "    if data_table:\n",
    "        # Initialize lists to store the scraped data\n",
    "        ranks = []\n",
    "        states = []\n",
    "        gsdp_1819 = []\n",
    "        gsdp_1920 = []\n",
    "        share_1819 = []\n",
    "        gdp_billion = []\n",
    "\n",
    "        # Iterate through the rows of the table and extract data\n",
    "        for row in data_table.find_all(\"tr\")[1:]:  # Skip the header row\n",
    "            columns = row.find_all(\"td\")\n",
    "            ranks.append(columns[0].text.strip())\n",
    "            states.append(columns[1].text.strip())\n",
    "            gsdp_1819.append(columns[2].text.strip())\n",
    "            gsdp_1920.append(columns[3].text.strip())\n",
    "            share_1819.append(columns[4].text.strip())\n",
    "            gdp_billion.append(columns[5].text.strip())\n",
    "\n",
    "        # Create or open the q3.txt file and write the extracted information\n",
    "        with open(\"q3.txt\", \"w\") as txt_file:\n",
    "            for i in range(len(ranks)):\n",
    "                txt_file.write(f\"Rank: {ranks[i]}\\n\")\n",
    "                txt_file.write(f\"State: {states[i]}\\n\")\n",
    "                txt_file.write(f\"GSDP(18-19): {gsdp_1819[i]}\\n\")\n",
    "                txt_file.write(f\"GSDP(19-20): {gsdp_1920[i]}\\n\")\n",
    "                txt_file.write(f\"Share(18-19): {share_1819[i]}\\n\")\n",
    "                txt_file.write(f\"GDP($ billion): {gdp_billion[i]}\\n\\n\")\n",
    "\n",
    "        print(\"Scraped data has been saved to q3.txt.\")\n",
    "    else:\n",
    "        print(\"Table containing GDP data not found on the page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d08c5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved in 'q4.txt' file.\n"
     ]
    }
   ],
   "source": [
    "#Question 4\n",
    "\n",
    "# URL of the GitHub trending page\n",
    "url = \"https://github.com/trending\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the list of trending repositories\n",
    "    repo_list = soup.find_all('article', class_='Box-row')\n",
    "\n",
    "    # Initialize an empty string to store all repository information\n",
    "    all_repo_details = \"\"\n",
    "\n",
    "    # Loop through the repositories and extract the required information\n",
    "    for repo in repo_list:\n",
    "        # Extract the repository title\n",
    "        repo_title_elem = repo.find('h1')\n",
    "        repo_title = repo_title_elem.text.strip() if repo_title_elem else 'N/A'\n",
    "\n",
    "        # Extract the repository description\n",
    "        repo_desc_elem = repo.find('p', class_='mb-1')\n",
    "        repo_desc = repo_desc_elem.text.strip() if repo_desc_elem else 'No description available'\n",
    "\n",
    "        # Extract the contributors count\n",
    "        contributors_count_elem = repo.find('a', href=True, attrs={'aria-label': 'Stargazers'})\n",
    "        contributors_count = contributors_count_elem['aria-label'].split()[-2] if contributors_count_elem else '0'\n",
    "\n",
    "        # Extract the language used\n",
    "        language_elem = repo.find('span', class_='d-inline-block ml-0 mr-3')\n",
    "        language = language_elem.text.strip() if language_elem else 'Not specified'\n",
    "\n",
    "        # Create a string with the repository information\n",
    "        repo_details = f\"A) Repository title: {repo_title}\\n\" \\\n",
    "                      f\"B) Repository description: {repo_desc}\\n\" \\\n",
    "                      f\"C) Contributors count: {contributors_count}\\n\" \\\n",
    "                      f\"D) Language used: {language}\\n\"\n",
    "\n",
    "        # Append the repository details to the existing string\n",
    "        all_repo_details += repo_details + \"\\n\"\n",
    "\n",
    "    # Save all repository information in a text file (q4.txt)\n",
    "    with open(\"q4.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(all_repo_details)\n",
    "\n",
    "    print(\"Data has been scraped and saved in 'q4.txt' file.\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the GitHub trending page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d40a905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved in 'q5.txt' file.\n"
     ]
    }
   ],
   "source": [
    "#Question 5 \n",
    "\n",
    "# URL of the Billboard Hot 100 page\n",
    "url = \"https://www.billboard.com/charts/hot-100\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the list of songs\n",
    "    songs_list = soup.find_all('li', class_='chart-list__element')\n",
    "\n",
    "    # Initialize an empty string to store all song information\n",
    "    all_song_details = \"\"\n",
    "\n",
    "    # Loop through the songs and extract the required information\n",
    "    for song in songs_list:\n",
    "        # Extract the song name\n",
    "        song_name = song.find('span', class_='chart-element__information__song').text.strip()\n",
    "\n",
    "        # Extract the artist name\n",
    "        artist_name = song.find('span', class_='chart-element__information__artist').text.strip()\n",
    "\n",
    "        # Extract the last week rank\n",
    "        last_week_rank_elem = song.find('span', class_='chart-element__meta text--last')\n",
    "        last_week_rank = last_week_rank_elem.text.strip() if last_week_rank_elem else 'N/A'\n",
    "\n",
    "        # Extract the peak rank\n",
    "        peak_rank_elem = song.find('span', class_='chart-element__meta text--peak')\n",
    "        peak_rank = peak_rank_elem.text.strip() if peak_rank_elem else 'N/A'\n",
    "\n",
    "        # Extract the weeks on board\n",
    "        weeks_on_board_elem = song.find('span', class_='chart-element__meta text--week')\n",
    "        weeks_on_board = weeks_on_board_elem.text.strip() if weeks_on_board_elem else 'N/A'\n",
    "\n",
    "        # Create a string with the song information\n",
    "        song_details = f\"A) Song name: {song_name}\\n\" \\\n",
    "                       f\"B) Artist name: {artist_name}\\n\" \\\n",
    "                       f\"C) Last week rank: {last_week_rank}\\n\" \\\n",
    "                       f\"D) Peak rank: {peak_rank}\\n\" \\\n",
    "                       f\"E) Weeks on board: {weeks_on_board}\\n\"\n",
    "\n",
    "        # Append the song details to the existing string\n",
    "        all_song_details += song_details + \"\\n\"\n",
    "\n",
    "    # Save all song information in a text file (q5.txt)\n",
    "    with open(\"q5.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(all_song_details)\n",
    "\n",
    "    print(\"Data has been scraped and saved in 'q5.txt' file.\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the Billboard Hot 100 page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c0ede84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data for the top 10 books has been saved to q6.txt.\n"
     ]
    }
   ],
   "source": [
    "#Question 6\n",
    "\n",
    "# Define the URL of The Guardian source\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the novel details\n",
    "    table = soup.find(\"table\")\n",
    "\n",
    "    if table:\n",
    "        # Initialize lists to store the scraped data\n",
    "        book_names = []\n",
    "        author_names = []\n",
    "        volumes_sold = []\n",
    "        publishers = []\n",
    "        genres = []\n",
    "\n",
    "        # Iterate through the rows of the table and extract data for the top 10 entries\n",
    "        rows = table.find_all(\"tr\")[1:11]  # Skip the header row and limit to the first 10 rows\n",
    "        for row in rows:\n",
    "            columns = row.find_all(\"td\")\n",
    "            if len(columns) >= 5:  # Ensure there are enough columns\n",
    "                book_names.append(columns[0].text.strip())\n",
    "                author_names.append(columns[1].text.strip())\n",
    "                volumes_sold.append(columns[2].text.strip())\n",
    "                publishers.append(columns[3].text.strip())\n",
    "                genres.append(columns[4].text.strip())\n",
    "\n",
    "        # Save the scraped data to q6.txt\n",
    "        with open(\"q6.txt\", \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            for i in range(len(book_names)):\n",
    "                txt_file.write(f\"Book Name: {book_names[i]}\\n\")\n",
    "                txt_file.write(f\"Author Name: {author_names[i]}\\n\")\n",
    "                txt_file.write(f\"Volumes Sold: {volumes_sold[i]}\\n\")\n",
    "                txt_file.write(f\"Publisher: {publishers[i]}\\n\")\n",
    "                txt_file.write(f\"Genre: {genres[i]}\\n\\n\")\n",
    "\n",
    "        print(\"Scraped data for the top 10 books has been saved to q6.txt.\")\n",
    "    else:\n",
    "        print(\"Table containing novel details not found on the page.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48431b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and saved in 'q7.txt' file.\n"
     ]
    }
   ],
   "source": [
    "#Question 7 \n",
    "\n",
    "# URL of the IMDb list of most-watched TV series\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the list of TV series\n",
    "    series_list = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "    # Initialize an empty string to store all series information\n",
    "    all_series_details = \"\"\n",
    "\n",
    "    # Loop through the TV series and extract the required information\n",
    "    for series in series_list:\n",
    "        # Extract the name\n",
    "        name = series.find('h3', class_='lister-item-header').a.text.strip()\n",
    "\n",
    "        # Extract the year span\n",
    "        year_span = series.find('span', class_='lister-item-year').text.strip()\n",
    "\n",
    "        # Extract the genre\n",
    "        genre = series.find('span', class_='genre').text.strip()\n",
    "\n",
    "        # Extract the run time\n",
    "        run_time = series.find('span', class_='runtime').text.strip()\n",
    "\n",
    "        # Extract the ratings\n",
    "        ratings = series.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "\n",
    "        # Extract the votes\n",
    "        votes_elem = series.find('span', attrs={'name': 'rk'})\n",
    "        votes = votes_elem['data-value'] if votes_elem else 'N/A'\n",
    "\n",
    "        # Create a string with the series information\n",
    "        series_details = f\"A) Name: {name}\\n\" \\\n",
    "                        f\"B) Year span: {year_span}\\n\" \\\n",
    "                        f\"C) Genre: {genre}\\n\" \\\n",
    "                        f\"D) Run time: {run_time}\\n\" \\\n",
    "                        f\"E) Ratings: {ratings}\\n\" \\\n",
    "                        f\"F) Votes: {votes}\\n\"\n",
    "\n",
    "        # Append the series details to the existing string\n",
    "        all_series_details += series_details + \"\\n\"\n",
    "\n",
    "    # Save all series information in a text file (q7.txt)\n",
    "    with open(\"q7.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(all_series_details)\n",
    "\n",
    "    print(\"Data has been scraped and saved in 'q7.txt' file.\")\n",
    "else:\n",
    "    print(\"Failed to fetch data from the IMDb page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfa159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
